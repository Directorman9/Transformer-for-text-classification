{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfomer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1VRUf4z0KL1PjdvQx5_PXBmCwDLH0ifu4","authorship_tag":"ABX9TyPIXUONO24BzwXVrmrElres"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gtIfsGAXcB42"},"source":["This is an implemetation of a custom transformer for text classification. It classifies long text (500-1000 words) into binary sentiment classes (positive or negative)."]},{"cell_type":"code","metadata":{"id":"i-M0z0q1RlvX"},"source":["import torch, pickle, math\n","import numpy as np\n","import pandas as pd\n","from torchtext.vocab import Vectors\n","from torchtext import data\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import *\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","#from nltk.corpus.stopwords import words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkpkdRJTdl3d"},"source":["The dataset\n","\n"]},{"cell_type":"code","metadata":{"id":"86RyylpFdnih","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606464191714,"user_tz":-180,"elapsed":2158,"user":{"displayName":"Director MAN9","photoUrl":"","userId":"05894045312788966826"}},"outputId":"70059737-047e-4af9-b6bb-6af8aa8e2325"},"source":["path='drive/My Drive/Colab Notebooks/Colab Datasets/longtext/training_set.csv'\n","train_df = pd.read_csv(path, sep=',', delimiter=None, header='infer')\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>One of the other reviewers has mentioned that ...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I thought this was a wonderful way to spend ti...</td>\n","      <td>positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Basically there's a family where a little boy ...</td>\n","      <td>negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n","      <td>positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review sentiment\n","0  One of the other reviewers has mentioned that ...  positive\n","1  A wonderful little production. <br /><br />The...  positive\n","2  I thought this was a wonderful way to spend ti...  positive\n","3  Basically there's a family where a little boy ...  negative\n","4  Petter Mattei's \"Love in the Time of Money\" is...  positive"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"hYEoyjYVewuI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606464191716,"user_tz":-180,"elapsed":2152,"user":{"displayName":"Director MAN9","photoUrl":"","userId":"05894045312788966826"}},"outputId":"45e0085b-aef2-428b-a779-1ebd46fc4939"},"source":["train_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(45000, 2)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"RiLDWtQXgU42"},"source":["def preprocess(x):\n","    if len(x) < max_seq_len:\n","       remain = max_seq_len-len(x)\n","       lst = ['<pad>']*remain\n","       x.extend(lst)\n","       return x\n","    else:\n","       lst = x[0:max_seq_len]\n","       return lst\n","\n","\n","class PositionalEncoding(nn.Module):\n","\n","  def __init__(self, d_model, dropout=0.1, max_len=5000):\n","      super(PositionalEncoding, self).__init__()\n","      self.dropout = nn.Dropout(p=dropout)\n","\n","      pe = torch.zeros(max_len, d_model)\n","      position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","      div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","      pe[:, 0::2] = torch.sin(position * div_term)\n","      pe[:, 1::2] = torch.cos(position * div_term)\n","      pe = pe.unsqueeze(0).transpose(0, 1)\n","      self.register_buffer('pe', pe)\n","\n","  def forward(self, x):\n","      x = x + self.pe[:x.size(0), :]\n","      return self.dropout(x)\n","\n","\n","class TransformerModel(nn.Module):\n","\n","  def __init__(self, glove, vocab_size, embed_dim, nhead, nlayers, dropout, attn_dim, out_dim): \n","      super(TransformerModel, self).__init__()\n","      self.embed_dim = embed_dim\n","      self.embed_layer = nn.Embedding.from_pretrained(glove, freeze=True)\n","      self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n","      self.transformer_encoder = TransformerEncoder(TransformerEncoderLayer(embed_dim, nhead, attn_dim, dropout), nlayers)\n","      self.out_layer = nn.Linear(embed_dim*max_seq_len, out_dim)\n","\n","  def forward(self, inpt): \n","      embeds = self.embed_layer(inpt) * math.sqrt(self.embed_dim)        \n","      embeds = self.pos_encoder(embeds)\n","      out = self.transformer_encoder(embeds)\n","      out = out.view(out.shape[0], -1) \n","      out = self.out_layer(out)\n","      scores = F.log_softmax(out, dim=1) \n","      return scores\n","\n","\n","#Training the model\n","def train(model,iterator):\n","    model.train()\n","    losses = []\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","        modeloutput = model(batch.review)\n","        loss = lossFunction(modeloutput, batch.sentiment)\n","        loss.backward()\n","        optimizer.step()\n","        losses.append(loss.item())\n","    return losses\n","\n","\n","#Evaluating the model\n","def evaluate(model,iterator):\n","    model.eval()\n","    actuals = []\n","    predictions = []\n","    with torch.no_grad():\n","         for batch in iterator:\n","             modeloutput = model(batch.review)\n","             prediction = modeloutput.argmax(dim=1, keepdim=True)\n","             actuals.extend(batch.sentiment)\n","             predictions.extend(prediction)\n","    return [i.item() for i in actuals], [i.item() for i in predictions]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_yiBRNNgP5G"},"source":["SEED = 1234\n","torch.manual_seed(SEED)\n","max_seq_len = 60"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTVa474GggNQ"},"source":["#Define columns of the dataframe that will hold the data\n","text = data.Field(preprocessing=preprocess, batch_first=True)\n","label = data.LabelField()\n","\n","#Map dataframe columns to columns in the csv file.\n","colmapper = [('review',text),('sentiment',label)]\n","\n","#Load data\n","train_data, val_data, test_data = data.TabularDataset.splits( \n","     path='drive/My Drive/Colab Notebooks/Colab Datasets/longtext', \n","     train='training_set.csv', \n","     validation='validation_set.csv', \n","     test='test_set.csv', \n","     format='csv',\n","     fields=colmapper,\n","     skip_header = True)\n","\n","#Explore what is loaded.\n","#print(vars(train_data.examples[0])['review'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4SEWLQTgn_Z"},"source":["#Build vocabulary\n","vectors = Vectors(name='drive/My Drive/Colab Notebooks/Colab Datasets/glove2B100d.txt', cache='./')\n","text.build_vocab(train_data, vectors=vectors, unk_init = torch.Tensor.normal_)\n","label.build_vocab(train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Is4coxwxgs0-"},"source":["#Create iterators\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, val_data, test_data), \n","    batch_size = 32,\n","    sort_key=lambda x: data.interleave_keys(len(x.review), len(x.sentiment)),\n","    sort_within_batch = False,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q690og8mgw8f"},"source":["#Define model parameters\n","vocab_size = len(text.vocab)\n","embed_dim = 100\n","nhead = 2\n","nlayers = 2\n","dropout = 0.1\n","attn_dim = 100\n","out_dim = len(label.vocab)\n","model = TransformerModel(text.vocab.vectors,vocab_size,embed_dim,nhead,nlayers,dropout,attn_dim,out_dim).to(device)\n","lossFunction = nn.NLLLoss()\n","optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad == True], lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6sfOKhRmg2ud"},"source":["#Train\n","losses = []\n","for epoch in range(0, 10):\n","    epoch_losses = train(model, train_iterator)\n","    losses.append(sum(epoch_losses))\n","np.savetxt(\"losses.csv\", losses, delimiter=\",\")\n","pickle.dump(model, open(\"bestModel.p\", \"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWRMmOv8g6pj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606464322372,"user_tz":-180,"elapsed":132766,"user":{"displayName":"Director MAN9","photoUrl":"","userId":"05894045312788966826"}},"outputId":"7062b534-79dd-4ce7-c0ca-bb6814cf299a"},"source":["#test the best model\n","model = pickle.load(open(\"bestModel.p\", \"rb\"))\n","actuals, predictions = evaluate(model, test_iterator)\n","print('Confusion matrix:')\n","print(confusion_matrix(actuals, predictions))\n","print('F1 score: %f' % f1_score(actuals, predictions, average='micro'))\n","print('Accuracy score: %f' % accuracy_score(actuals, predictions))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Confusion matrix:\n","[[190  62]\n"," [ 74 174]]\n","F1 score: 0.728000\n","Accuracy score: 0.728000\n"],"name":"stdout"}]}]}